# Pulse ğŸš€

**Real-time Trend Intelligence Platform**

AI-powered trend monitoring and analysis system designed to identify emerging topics before they go mainstream, with predictive capabilities for 24-48h forecasting.

---

## ğŸ¯ Overview

Pulse aggregates content from multiple sources (RSS, Reddit, YouTube, GDELT, Hacker News), processes it with NLP and ML to detect emerging trends, calculate proprietary "Pulse" metrics (volume, velocity, spread, authority, novelty), and forecast which topics will become mainstream in the next 24-48 hours.

### Key Features

- âœ… **Multi-Source Ingestion**: RSS, Reddit, YouTube, GDELT, Hacker News
- âœ… **Topic Discovery**: BERTopic with multilingual embeddings
- âœ… **Pulse Metrics**: Volume, Velocity, Spread, Authority, Novelty, Variance
- âœ… **On-Demand Search**: Research agent for specific topics
- ğŸš§ **Forecasting**: ETS/Holt-Winters for trend prediction (in development)
- ğŸš§ **Alert System**: Email/Slack notifications (in development)
- ğŸš§ **Report Generator**: PDF executive summaries (in development)

---

## ğŸ“ Project Structure

```
Pulse/
â”œâ”€â”€ backend/                 # Python Backend (FastAPI)
â”‚   â”œâ”€â”€ api/                 # REST API endpoints
â”‚   â”œâ”€â”€ config/              # Configuration (settings.py)
â”‚   â”œâ”€â”€ jobs/                # Scheduled jobs (Celery)
â”‚   â”œâ”€â”€ models/              # Database models (SQLAlchemy + Pydantic)
â”‚   â”œâ”€â”€ scrapers/            # Modular web scrapers
â”‚   â”‚   â”œâ”€â”€ base_scraper.py  # Base interface
â”‚   â”‚   â”œâ”€â”€ ansa_scraper.py  # ANSA news
â”‚   â”‚   â”œâ”€â”€ reddit_scraper.py # Reddit
â”‚   â”‚   â”œâ”€â”€ hackernews_scraper.py # Hacker News
â”‚   â”‚   â””â”€â”€ registry.py      # Scraper registry
â”‚   â”œâ”€â”€ services/            # Business logic (NLP, ML, forecasting)
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â””â”€â”€ test_scrapers.py     # CLI tool for testing scrapers
â”‚
â”œâ”€â”€ frontend/                # React + Vite Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx          # Main component
â”‚   â”‚   â”œâ”€â”€ main.jsx         # Entry point
â”‚   â”‚   â””â”€â”€ index.css        # Tailwind CSS
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”‚
â”œâ”€â”€ data/                    # Data storage (gitignored)
â”œâ”€â”€ docs/                    # Documentation
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+**
- **Node.js 18+**
- **PostgreSQL 14+**
- **Redis** (per Celery)
- **MongoDB** (opzionale, per content store)

### 1. Setup Backend

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your credentials (DB, Reddit API, etc)

# Initialize database
# Make sure PostgreSQL is running
python -c "from models.database import init_db; init_db()"

# Start development server
uvicorn main:app --reload --port 8000
```

### 2. Setup Frontend

```bash
cd frontend

# Install dependencies
npm install

# Start dev server
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) in your browser.

### 3. Test Scrapers

```bash
cd backend
python test_scrapers.py "intelligenza artificiale" --sources ansa reddit hackernews --max-pages 3
```

---

## ğŸ—ºï¸ Roadmap

### âœ… **Week 1-2: Ingestion & Analytics Base** (Completed)
- [x] Project structure setup
- [x] Modular scrapers (ANSA, Reddit, HackerNews)
- [x] Database schema (PostgreSQL)
- [x] Base UI with React wireframe
- [ ] Parser/Normalizer + Deduplication
- [ ] Language Detection/Geo tagging
- [ ] Job scheduler (Celery)

### ğŸš§ **Week 3: Topic & Pulse** (In Progress)
- [ ] Multilingual embeddings (sentence-transformers)
- [ ] BERTopic integration
- [ ] Pulse metrics calculation (volume, velocity, spread, etc)
- [ ] PulseScore formula
- [ ] API endpoints for Top Topics
- [ ] Topic details with citations (RAG)

### ğŸ“… **Week 4: On-Demand Search**
- [ ] Search agent (query generation)
- [ ] Ephemeral index for custom searches
- [ ] Multi-source result fusion
- [ ] UI: search box with live/on-demand badges

### ğŸ“… **Week 5: Forecast MVP**
- [ ] ETS/Holt-Winters on topics with sufficient history
- [ ] Confidence bands + heuristic fallback
- [ ] sMAPE metrics for internal evaluation

### ğŸ“… **Week 6: Alerts & Reports**
- [ ] Alert rules (country/category/threshold)
- [ ] Email/Slack webhooks
- [ ] PDF report generator (executive summary)
- [ ] Quality logging (accuracy, lead-time)

### ğŸ“… **Month 2-3: Advanced Prediction**
- [ ] Labeled historical data collection
- [ ] 48h mainstream classifier
- [ ] Sector-specific threshold tuning
- [ ] Hardening (job monitoring, retry, API quotas)

---

## ğŸ§ª Testing & Development

### Testing Scrapers

```bash
# Test single scraper
python test_scrapers.py "bitcoin" --sources reddit --max-pages 2

# Test multiple scrapers with CSV output
python test_scrapers.py "AI" --sources ansa hackernews reddit --output results.csv --days-back 7
```

### API Testing

With backend running:

```bash
# Health check
curl http://localhost:8000/api/v1/health

# TODO: More endpoints when implemented
```

---

## ğŸ› ï¸ Tech Stack

### Backend
- **FastAPI** - Web framework
- **SQLAlchemy** - ORM (PostgreSQL)
- **Celery** - Task queue
- **Redis** - Cache & message broker
- **BeautifulSoup** - Web scraping
- **PRAW** - Reddit API
- **sentence-transformers** - Embeddings multilingue
- **BERTopic** - Topic modeling
- **statsmodels/prophet** - Time series forecasting

### Frontend
- **React 18** - UI framework
- **Vite** - Build tool
- **Tailwind CSS** - Styling
- **Lucide React** - Icons
- **Axios** - HTTP client

### Infrastructure
- **PostgreSQL** - Database relazionale
- **MongoDB** - Content store (opzionale)
- **Docker** - Containerization (TODO)

---

## ğŸ“Š Database Schema

### Articles Table
```sql
- id (PK)
- source (ansa, reddit, youtube, gdelt, hn)
- source_id (unique)
- title, content, url
- published_at, scraped_at
- country, language, sector
- engagement_score, authority_score
- embedding_vector (JSON), topic_id
- sentiment_score
- raw_metadata (JSON)
```

### Topics Table
```sql
- id (PK), topic_id (BERTopic ID)
- label, keywords (JSON), description
- pulse_score, volume, velocity, spread
- authority, novelty, variance, sentiment_avg
- country, sector
- first_seen, last_updated
- history (JSON) # per forecasting
```

---

## ğŸ”‘ Environment Variables

Create `backend/.env` based on `.env.example`:

```bash
# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/pulse_db
MONGO_URL=mongodb://localhost:27017
REDIS_URL=redis://localhost:6379/0

# Reddit API (get from https://www.reddit.com/prefs/apps)
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_secret
REDDIT_USER_AGENT=PulseBot/1.0

# GDELT & YouTube (optional)
# GDELT_API_KEY=
# YOUTUBE_API_KEY=

# Scraping
SCRAPING_INTERVAL_MINUTES=10
MAX_PAGES_PER_SOURCE=10

# ML
EMBEDDINGS_MODEL=sentence-transformers/paraphrase-multilingual-mpnet-base-v2
MIN_TOPIC_SIZE=5
```

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file.

---

## ğŸ“§ Contact

For questions or support, open an Issue on GitHub.

---

**Status**: MVP in development (Week 2/12)
